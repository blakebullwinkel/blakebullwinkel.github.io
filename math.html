<!doctype html>
<html lang="en">
<head>
  <title>Math Notes</title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="icon" type="image/png" href="/static/marble.png">
  <link rel="stylesheet" type="text/css" href="styles.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

  <h1>Math Notes</h1>
  <p>
    I am currently working through the <a href="https://mml-book.com">
    Mathematics for Machine Learning</a> textbook by Deisenroth et al. 
    and use this space to explain things in my own words, which I find 
    to be the best test of understanding. If you find any mistakes, 
    please let me know!
  </p>

  <h2>Linear Algebra</h2>
  <p>
    A system of linear equations can be compactly represented by 
    \(A \boldsymbol{x} = \boldsymbol{b}\) and will either have no 
    solutions, exactly one solution, or infinitely many solutions. 
    For example, the system

    $$
    \begin{aligned}
    x_1 + x_2 + x_3 &= 3 \\
    x_1 - x_2 + 2x_3 &= 2 \\
    2x_1 + 3x_3 &= 5
    \end{aligned}
    $$

    can be written as 

    $$
    \begin{bmatrix}
    1 & 1 & 1 \\
    1 & -1 & 2 \\
    2 & 0 & 3
    \end{bmatrix}
    \begin{bmatrix}
    x_1 \\
    x_2 \\
    x_3
    \end{bmatrix} = 
    \begin{bmatrix}
    3 \\
    2 \\
    5
    \end{bmatrix}
    $$
    Gaussian elimination is an algorithm that enables us to solve
    these systems by putting the matrix \(A\) into row echelon form
    or (better yet) <i>reduced</i> row echelon form. In this example, we would 
    construct the augmented matrix 
    
    $$
    \left[\begin{array}{rrr|r}
    1 & 1 & 1 & 3 \\
    1 & -1 & 2 & 2 \\
    2 & 0 & 3 & 5
    \end{array}\right]
    $$

    and perform elementary row operations until we obtain

    $$
    \left[\begin{array}{rrr|r}
    1 & 0 & \frac{3}{2} & \frac{5}{2} \\
    0 & 1 & -\frac{1}{2} & \frac{1}{2} \\
    0 & 0 & 0 & 0
    \end{array}\right],
    $$

  which tells us that \(x_1+\frac{3}{2}x_3 = \frac{5}{2}\) and 
  \(x_2-\frac{1}{2}x_3 = \frac{1}{2}\). In this case, we have 
  only two equations with three unknowns, so we introduce a 
  free variable \(t=x_3\). Solving for \(x_1\) and \(x_2\), we 
  obtain 

    $$
    \begin{bmatrix}
    x_1 \\
    x_2 \\
    x_3
    \end{bmatrix} = 
    \begin{bmatrix}
    \frac{5-3t}{2} \\
    \frac{1+t}{2} \\
    t
    \end{bmatrix} = 
    \begin{bmatrix}
    \frac{5}{2} \\
    \frac{1}{2} \\
    0
    \end{bmatrix} + t
    \begin{bmatrix}
    \frac{-3}{2} \\
    \frac{1}{2} \\
    1
    \end{bmatrix}
    $$
    where \(t \in \mathbb{R}\). Therefore, this system has infinitely 
    many solutions. Recognizing that this solution describes a line
    in \(\mathbb{R}^3\), we can imagine that there is an infinite line
    of solutions that satisfy our equation.
  </p>
  <p>
    For a more intuitive understanding of what that means, it
    is helpful to remember that \(A \in \mathbb{R}^{m \times n} \) 
    represents a linear transformation \( T(\boldsymbol{x}): \mathbb{R}^n 
    \rightarrow \mathbb{R}^m \). In other words, the equation 
    \(A\boldsymbol{x} = \boldsymbol{b}\) is really asking, "what vector 
    \(\boldsymbol{x}\) will land on \(\boldsymbol{b}\) after the 
    transformation \(A\) is applied?" Our solution says that an infinite 
    line of vectors will land on the vector \(\boldsymbol{b}\).
  </p>
  <p>
    Let's dig a little deeper by thinking for a 
    moment about just an arbitrary vector \([x_1,x_2,x_3]^\top \in \mathbb{R}^3\).
    Another way to express this vector is as a linear combination of three 
    more fundamental vectors:
    $$
    \begin{bmatrix}
    x_1 \\
    x_2 \\
    x_3
    \end{bmatrix} = x_1
    \begin{bmatrix}
    1 \\
    0 \\
    0
    \end{bmatrix} + x_2
    \begin{bmatrix}
    0 \\
    1 \\
    0
    \end{bmatrix} + x_3
    \begin{bmatrix}
    0 \\
    0 \\
    1
    \end{bmatrix}
    $$
  </p>
  <p>  
    These vectors
    \(\boldsymbol{e}_1=[1,0,0]^\top, \boldsymbol{e}_2=[0,1,0]^\top, 
    \boldsymbol{e}_3=[0,0,1]^\top \) are called standard basis vectors.
    Going back to our example above, the matrix \(A \in \mathbb{R}^3\)
    represents a linear transformation \(T(\boldsymbol{x}):\mathbb{R}^3
    \rightarrow \mathbb{R}^3\). In other words, \(A\) maps every vector in 
    \(\mathbb{R}^3\) to a new vector in \(\mathbb{R}^3\), including the 
    standard basis vectors. Because the transformation is <i>linear</i>,
    we can write that 
    $$
    T\left(\begin{bmatrix}
      x_1 \\
      x_2 \\
      x_3
      \end{bmatrix}
    \right) =
    x_1 T\left(
      \begin{bmatrix}
      1 \\
      0 \\
      0
      \end{bmatrix}
    \right) +
    x_2 T\left(
      \begin{bmatrix}
      0 \\
      1 \\
      0
      \end{bmatrix}
    \right) +
    x_3 T\left(
      \begin{bmatrix}
      0 \\
      0 \\
      1
      \end{bmatrix}
    \right)
    $$
  </p>
  <p>
    This tells us that the transformed version of any vector 
    \([x_1,x_2,x_3]^\top\) can be found by scaling the <i>transformed</i>
    standard basis vectors by \(x_1, x_2, x_3\) and adding them up.
    Therefore, all the information about \(T(\boldsymbol{x})\) is encoded by
    the transformed versions of the standard basis vectors. Given that 
    \(T(\boldsymbol{x})\) is also described by the matrix multiplication 
    \(A\boldsymbol{x}\), it follows from the above that the columns of \(A\)
    are precisely the transformed standard basis vectors.
  </p>
  <p>
    In our example above, therefore, the transformation matrix 
    $$
    A = 
    \begin{bmatrix}
    1 & 1 & 1 \\
    1 & -1 & 2 \\
    2 & 0 & 3
    \end{bmatrix}
    $$
    maps the standard basis vectors to \([1,1,2]^\top, [1,-1,0]^\top, 
    [1,2,3]^\top \) and, as we found in the example above, it maps 
    an infinite line of vectors onto \(\boldsymbol{b}=[3,2,5]^\top\).
  </p>
  <p>
    You might be wondering, why did we get infinitely many solutions?
    When would we have no solutions, or exactly one solution? To answer
    these questions, it will be helpful to define a number of concepts, 
    including vector spaces, linear (in)dependence, span, and bases.
  </p>
  <p>
    Let's start with vector spaces. A real-valued vector space \(V\) is a set 
    of vectors with two operations: addition between two vectors, and 
    multiplication between a vector and a scalar.
  </p>
  <p>
    Consider a set of vectors \(\boldsymbol{v}_1,\boldsymbol{v}_2,\dots 
    \boldsymbol{v}_k \in V\). These vectors are said to be linearly 
    independent if

    $$
    c_1 \boldsymbol{v}_1 + c_2 \boldsymbol{v}_2 + \dots + c_k 
    \boldsymbol{v}_k = \boldsymbol{0}
    $$

    only holds for \(c_1=c_2=\dots=c_k=0\). If, however, any of the 
    vectors can be expressed as a non-trivial linear combination of the 
    others, then the vectors are said to be linearly <i>dependent</i>.
  </p>
  <p>
    Returning to our example, let's check whether the columns of \(A\) are 
    linearly independent. To do this, we have to solve yet another system 
    of equations. In particular, we want to know whether

    $$
    \begin{bmatrix}
    1 & 1 & 1 \\
    1 & -1 & 2 \\
    2 & 0 & 3
    \end{bmatrix}
    \begin{bmatrix}
    c_1 \\
    c_2 \\
    c_3
    \end{bmatrix} = 
    \begin{bmatrix}
    0 \\
    0 \\
    0
    \end{bmatrix},
    $$

    is solved only by \(c_1=c_2=c_3=0\). By definition, this would imply 
    that the columns of \(A\) are linearly independent. Using Gaussian elimination, 
    we found that

    $$
    \left[\begin{array}{rrr|r}
    1 & 0 & \frac{3}{2} & 0 \\
    0 & 1 & -\frac{1}{2} & 0 \\
    0 & 0 & 0 & 0
    \end{array}\right]
    \implies

    \begin{bmatrix}
    c_1 \\
    c_2 \\
    c_3
    \end{bmatrix} = t
    \begin{bmatrix}
    \frac{-3}{2} \\
    \frac{1}{2} \\
    1
    \end{bmatrix}
    $$

    where \(t \in \mathbb{R}\). Therefore, we <i>do not</i> require
    \(c_1=c_2=c_3=0\) to solve this system, and 
    so the columns of \(A\) are linearly dependent. Looking at the 
    columns of the reduced row echelon matrix, we can see that any column of
    \(A\) can be obtained by a linear combination of the other two columns.
  </p>
  <p>
    This brings us to our next key concept: span. While the columns of
    \(A\) are vectors in \(\mathbb{R}^3\), the fact that each vector 
    is obtained by a linear combination of the other two implies that the three 
    vectors exist on a single two-dimensional plane in \(\mathbb{R}^3\).
  </p>
  <p>
    Coming next: formal definition of span, basis, and eventually change of basis.
  </p>
</body>