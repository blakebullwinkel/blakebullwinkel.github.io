<!doctype html>
<html lang="en">
<head>
  <title>Math Notes</title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!-- <link rel="icon" type="image/png" href="/static/marble.png"> -->
  <link rel="stylesheet" type="text/css" href="styles.css">
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

  <h1>Math Notes</h1>
  <p>
    I am currently working through the <a href="https://mml-book.com">
    Mathematics for Machine Learning</a> textbook by Deisenroth et al. 
    and use this space to summarize my notes and organize my thoughts. 
    If you find any mistakes, please let me know!
  </p>

  <h2>Linear Algebra</h2>
  <p>
    A system of linear equations can be compactly represented by 
    \(A \boldsymbol{x} = \boldsymbol{b}\) and will either have no 
    solutions, exactly one solution, or infinitely many solutions. 
    For example, the system

    $$
    \begin{aligned}
    x_1 + x_2 + x_3 &= 3 \\
    x_1 - x_2 + 2x_3 &= 2 \\
    2x_1 + 3x_3 &= 5
    \end{aligned}
    $$

    can be written as 

    $$
    \begin{bmatrix}
    1 & 1 & 1 \\
    1 & -1 & 2 \\
    2 & 0 & 3
    \end{bmatrix}
    \begin{bmatrix}
    x_1 \\
    x_2 \\
    x_3
    \end{bmatrix} = 
    \begin{bmatrix}
    3 \\
    2 \\
    5
    \end{bmatrix}
    $$
    Gaussian elimination is an algorithm that enables us to solve
    these systems by putting the matrix \(A\) into row echelon form
    or (better yet) <i>reduced</i> row echelon form. In this example, we would 
    construct the augmented matrix 
    
    $$
    \left[\begin{array}{rrr|r}
    1 & 1 & 1 & 3 \\
    1 & -1 & 2 & 2 \\
    2 & 0 & 3 & 5
    \end{array}\right]
    $$

    and perform elementary row operations until we obtain

    $$
    \left[\begin{array}{rrr|r}
    1 & 0 & \frac{3}{2} & \frac{5}{2} \\
    0 & 1 & -\frac{1}{2} & \frac{1}{2} \\
    0 & 0 & 0 & 0
    \end{array}\right],
    $$

  which tells us that \(x_1+\frac{3}{2}x_3 = \frac{5}{2}\) and 
  \(x_2-\frac{1}{2}x_3 = \frac{1}{2}\). In this case, we have 
  only two equations with three unknowns, so we introduce a 
  free variable \(t=x_3\). Solving for \(x_1\) and \(x_2\), we 
  obtain 

    $$
    \begin{bmatrix}
    x_1 \\
    x_2 \\
    x_3
    \end{bmatrix} = 
    \begin{bmatrix}
    \frac{5-3t}{2} \\
    \frac{1+t}{2} \\
    t
    \end{bmatrix} = 
    \begin{bmatrix}
    \frac{5}{2} \\
    \frac{1}{2} \\
    0
    \end{bmatrix} + t
    \begin{bmatrix}
    \frac{-3}{2} \\
    \frac{1}{2} \\
    1
    \end{bmatrix}
    $$
    where \(t \in \mathbb{R}\). Therefore, this system has infinitely 
    many solutions. Recognizing that this solution describes a line
    in \(\mathbb{R}^3\), we can imagine that there is an infinite line
    of solutions that satisfy our equation.
  </p>
  <p>
    For a more intuitive understanding of what that means, it
    is helpful to remember that \(A \in \mathbb{R}^{m \times n} \) 
    represents a linear transformation \( T(\boldsymbol{x}): \mathbb{R}^n 
    \rightarrow \mathbb{R}^m \). In other words, the equation 
    \(A\boldsymbol{x} = \boldsymbol{b}\) is really asking, "what vector 
    \(\boldsymbol{x}\) will land on \(\boldsymbol{b}\) after the 
    transformation \(A\) is applied?" Our solution says that an infinite 
    line of vectors will land on the vector \(\boldsymbol{b}\).
  </p>
  <p>
    Without introducing some additional concepts, I find it hard to 
    visualize how that happens. For a deeper understanding, let's talk
    about column space, linear independence, and determinants.
  </p>
  <p>
    Everything you need to know about a transformation matrix is 
    contained within its column vectors. As Grant Sanderson engrained 
    in my memory through his highly acclaimed 
    <a href="https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">Essence of linear 
    algebra</a>, the column vectors of a matrix tell us where the
    standard basis vectors will land after the transformation matrix 
    is applied. 
  </p>
  <p>  
    In the example above, \(A \in \mathbb{R}^{3 \times 3}\) is a linear
    transformation that maps the standard basis vectors 
    \(\boldsymbol{e}_1=[1,0,0]^\top, \boldsymbol{e}_2=[0,1,0]^\top, 
    \boldsymbol{e}_3=[0,0,1]^\top \) to \( [1,1,2]^\top, [1,-1,0]^\top, 
    [1,2,3]^\top\). This doesn't provide much in the way of visual intuition,
    but we're getting there.
  </p>
  <p>
    A set of vectors \(\boldsymbol{v}_1,\boldsymbol{v}_2,\dots 
    \boldsymbol{v}_k\) is said to be linearly independent if the 
    equality

    $$
    c_1 \boldsymbol{v}_1 + c_2 \boldsymbol{v}_2 + \dots + c_k 
    \boldsymbol{v}_k = \boldsymbol{0}
    $$

    is only true for \(c_1=c_2=\dots=c_k=0\). What the heck does this 
    mean? It means that we can't write any of the vectors \(\boldsymbol{v}_1,
    \boldsymbol{v}_2,\dots \boldsymbol{v}_k\) as a linear combination of 
    the others. If we could, then we would say that this set of vectors 
    is linearly <i>dependent</i>.
  </p>
  <p>
    Let's think about how this is relevant to understanding a transformation 
    matrix \(A\).
  </p>
</body>